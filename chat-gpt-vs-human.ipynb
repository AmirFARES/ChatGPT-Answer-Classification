{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-22T11:55:27.729031Z","iopub.execute_input":"2023-02-22T11:55:27.729488Z","iopub.status.idle":"2023-02-22T11:55:35.883334Z","shell.execute_reply.started":"2023-02-22T11:55:27.729398Z","shell.execute_reply":"2023-02-22T11:55:35.882313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:35.885119Z","iopub.execute_input":"2023-02-22T11:55:35.886000Z","iopub.status.idle":"2023-02-22T11:55:35.895455Z","shell.execute_reply.started":"2023-02-22T11:55:35.885961Z","shell.execute_reply":"2023-02-22T11:55:35.894108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/ml-olympiad-detect-chatgpt-answers/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/ml-olympiad-detect-chatgpt-answers/test.csv\",index_col='ID')\n\ntrain=train.dropna()\ntrain = train.astype({'ID':'int'})\ntrain = train.astype({'AI':'int'})\ntrain=train.set_index('ID')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:35.897226Z","iopub.execute_input":"2023-02-22T11:55:35.897709Z","iopub.status.idle":"2023-02-22T11:55:35.969639Z","shell.execute_reply.started":"2023-02-22T11:55:35.897651Z","shell.execute_reply":"2023-02-22T11:55:35.968504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:35.971947Z","iopub.execute_input":"2023-02-22T11:55:35.972882Z","iopub.status.idle":"2023-02-22T11:55:35.995769Z","shell.execute_reply.started":"2023-02-22T11:55:35.972844Z","shell.execute_reply":"2023-02-22T11:55:35.994012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:35.998546Z","iopub.execute_input":"2023-02-22T11:55:35.999168Z","iopub.status.idle":"2023-02-22T11:55:36.016289Z","shell.execute_reply.started":"2023-02-22T11:55:35.999113Z","shell.execute_reply":"2023-02-22T11:55:36.014811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 20 # lenght of articles\nembedding_dim = 50 #dim of dictionarry glove\n\ntokenizer=Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train['prompt'].values + train['answer'].values)\n\nprompt_sequences = tokenizer.texts_to_sequences(train['prompt'].values)\nanswer_sequences = tokenizer.texts_to_sequences(train['answer'].values)\n\nvocab_size = len(tokenizer.word_index) + 1\nword_index=tokenizer.word_index\npadded_prompt_sequences = pad_sequences(prompt_sequences, maxlen=max_length)\npadded_answer_sequences = pad_sequences(answer_sequences, maxlen=max_length)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:36.017624Z","iopub.execute_input":"2023-02-22T11:55:36.017984Z","iopub.status.idle":"2023-02-22T11:55:36.099090Z","shell.execute_reply.started":"2023-02-22T11:55:36.017953Z","shell.execute_reply":"2023-02-22T11:55:36.097806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=np.concatenate((padded_prompt_sequences, padded_answer_sequences), axis=1)\ny_train = train['AI'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:36.100646Z","iopub.execute_input":"2023-02-22T11:55:36.101068Z","iopub.status.idle":"2023-02-22T11:55:36.108203Z","shell.execute_reply.started":"2023-02-22T11:55:36.101034Z","shell.execute_reply":"2023-02-22T11:55:36.106669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout,RNN\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=40))\nmodel.add(LSTM(40))\nmodel.add(Dropout(0.69))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory=model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\nplot_history(history)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:36.109892Z","iopub.execute_input":"2023-02-22T11:55:36.110884Z","iopub.status.idle":"2023-02-22T11:55:51.529724Z","shell.execute_reply.started":"2023-02-22T11:55:36.110825Z","shell.execute_reply":"2023-02-22T11:55:51.528119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import RNN, Conv1D, MaxPooling1D,GlobalAveragePooling1D,GlobalMaxPooling1D, Conv2D,MaxPooling2D\n\nmodel_conv = Sequential()\nmodel_conv.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=40))\nmodel_conv.add(Conv1D(5,3, activation='relu')) #apply 10 filters \nmodel_conv.add(GlobalAveragePooling1D())\nmodel_conv.add(Dropout(0.4))\nmodel_conv.add(Dense(1, activation='sigmoid'))\n\nmodel_conv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory=model_conv.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:51.531152Z","iopub.execute_input":"2023-02-22T11:55:51.531758Z","iopub.status.idle":"2023-02-22T11:55:57.155245Z","shell.execute_reply.started":"2023-02-22T11:55:51.531720Z","shell.execute_reply":"2023-02-22T11:55:57.153931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:57.160804Z","iopub.execute_input":"2023-02-22T11:55:57.161176Z","iopub.status.idle":"2023-02-22T11:55:57.175747Z","shell.execute_reply.started":"2023-02-22T11:55:57.161146Z","shell.execute_reply":"2023-02-22T11:55:57.174384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prompt_sequences = tokenizer.texts_to_sequences(test['prompt'].values)\ntest_answer_sequences = tokenizer.texts_to_sequences(test['answer'].values)\n\n\ntest_padded_prompt_sequences = pad_sequences(test_prompt_sequences, maxlen=max_length)\ntest_padded_answer_sequences = pad_sequences(test_answer_sequences, maxlen=max_length)\n\nX_test=np.concatenate((test_padded_prompt_sequences, test_padded_answer_sequences), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:57.177509Z","iopub.execute_input":"2023-02-22T11:55:57.178633Z","iopub.status.idle":"2023-02-22T11:55:57.205774Z","shell.execute_reply.started":"2023-02-22T11:55:57.178586Z","shell.execute_reply":"2023-02-22T11:55:57.204650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:57.208507Z","iopub.execute_input":"2023-02-22T11:55:57.208923Z","iopub.status.idle":"2023-02-22T11:55:57.841994Z","shell.execute_reply.started":"2023-02-22T11:55:57.208889Z","shell.execute_reply":"2023-02-22T11:55:57.840965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:57.845400Z","iopub.execute_input":"2023-02-22T11:55:57.845924Z","iopub.status.idle":"2023-02-22T11:55:57.858021Z","shell.execute_reply.started":"2023-02-22T11:55:57.845875Z","shell.execute_reply":"2023-02-22T11:55:57.856539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# Initialize the tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the prompt and answer columns\ntokenizer.fit_on_texts(train['prompt'] + train['answer'])\n\n# Convert the text to sequences\nX_prompt = tokenizer.texts_to_sequences(train['prompt'])\nX_answer = tokenizer.texts_to_sequences(train['answer'])\n\n# Get the length of the longest sequence\nmax_length = max([len(sequence) for sequence in X_prompt + X_answer])\n\n# Pad the sequences to have the same length\nfrom keras_preprocessing.sequence import pad_sequences\n\nX_prompt = pad_sequences(X_prompt, maxlen=max_length, padding='post')\nX_answer = pad_sequences(X_answer, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:57.859648Z","iopub.execute_input":"2023-02-22T11:55:57.860739Z","iopub.status.idle":"2023-02-22T11:55:57.945656Z","shell.execute_reply.started":"2023-02-22T11:55:57.860688Z","shell.execute_reply":"2023-02-22T11:55:57.944682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# Initialize the tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the prompt and answer columns\ntokenizer.fit_on_texts(train['prompt'] + train['answer'])\n\n# Convert the text to sequences\nX_prompt = tokenizer.texts_to_sequences(train['prompt'])\nX_answer = tokenizer.texts_to_sequences(train['answer'])\n\n# Get the length of the longest sequence\nmax_length = max([len(sequence) for sequence in X_prompt + X_answer])\n\n# Pad the sequences to have the same length\nfrom keras_preprocessing.sequence import pad_sequences\n\nX_prompt = pad_sequences(X_prompt, maxlen=max_length, padding='post')\nX_answer = pad_sequences(X_answer, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:57.947175Z","iopub.execute_input":"2023-02-22T11:55:57.947798Z","iopub.status.idle":"2023-02-22T11:55:58.028545Z","shell.execute_reply.started":"2023-02-22T11:55:57.947761Z","shell.execute_reply":"2023-02-22T11:55:58.027139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# Initialize the tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the prompt and answer columns\ntokenizer.fit_on_texts(train['prompt'] + train['answer'])\n\n# Convert the text to sequences\nX_prompt = tokenizer.texts_to_sequences(train['prompt'])\nX_answer = tokenizer.texts_to_sequences(train['answer'])\n\n# Get the length of the longest sequence\nmax_length = max([len(sequence) for sequence in X_prompt + X_answer])\n\n# Pad the sequences to have the same length\nfrom keras_preprocessing.sequence import pad_sequences\n\nX_prompt = pad_sequences(X_prompt, maxlen=max_length, padding='post')\nX_answer = pad_sequences(X_answer, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:58.030485Z","iopub.execute_input":"2023-02-22T11:55:58.031034Z","iopub.status.idle":"2023-02-22T11:55:58.115932Z","shell.execute_reply.started":"2023-02-22T11:55:58.030981Z","shell.execute_reply":"2023-02-22T11:55:58.114616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSTM","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# Initialize the tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the prompt and answer columns\ntokenizer.fit_on_texts(train['prompt'] + train['answer'])\n\n# Convert the text to sequences\nX_prompt = tokenizer.texts_to_sequences(train['prompt'])\nX_answer = tokenizer.texts_to_sequences(train['answer'])\n\n# Get the length of the longest sequence\nmax_length = max([len(sequence) for sequence in X_prompt + X_answer])\n\n# Pad the sequences to have the same length\nfrom keras_preprocessing.sequence import pad_sequences\n\nX_prompt = pad_sequences(X_prompt, maxlen=max_length, padding='post')\nX_answer = pad_sequences(X_answer, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:58.117617Z","iopub.execute_input":"2023-02-22T11:55:58.118573Z","iopub.status.idle":"2023-02-22T11:55:58.202290Z","shell.execute_reply.started":"2023-02-22T11:55:58.118532Z","shell.execute_reply":"2023-02-22T11:55:58.201058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, LSTM, Input, concatenate\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Concatenate the embeddings\nconcatenated = concatenate([embedding_prompt, embedding_answer])\n\n# Define the LSTM layer\nlstm = LSTM(64, dropout=0.2)(concatenated)\n\n# Define the output layer\noutput = Dense(1, activation='sigmoid')(lstm)\n\n# Define the model\nmodel = Model(inputs=[input_prompt, input_answer], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=30, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:55:58.206658Z","iopub.execute_input":"2023-02-22T11:55:58.207108Z","iopub.status.idle":"2023-02-22T11:57:35.689705Z","shell.execute_reply.started":"2023-02-22T11:55:58.207070Z","shell.execute_reply":"2023-02-22T11:57:35.688814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, concatenate, Input\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Convolutional layers\nconv_prompt = Conv1D(32, 3, activation='relu')(embedding_prompt)\nconv_answer = Conv1D(32, 3, activation='relu')(embedding_answer)\n\n# Global max pooling layers\npooled_prompt = GlobalMaxPooling1D()(conv_prompt)\npooled_answer = GlobalMaxPooling1D()(conv_answer)\n\n# Concatenate the pooled layers\nconcatenated = concatenate([pooled_prompt, pooled_answer])\n\n# Dense layer\ndense = Dense(32, activation='relu')(concatenated)\n\n# Define the output layer\noutput = Dense(1, activation='sigmoid')(dense)\n\n# Define the model\nmodel = Model(inputs=[input_prompt, input_answer], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=30, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:57:35.691775Z","iopub.execute_input":"2023-02-22T11:57:35.692159Z","iopub.status.idle":"2023-02-22T11:57:51.056921Z","shell.execute_reply.started":"2023-02-22T11:57:35.692126Z","shell.execute_reply":"2023-02-22T11:57:51.055328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, SimpleRNN, Input, concatenate\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Concatenate the embeddings\nconcatenated = concatenate([embedding_prompt, embedding_answer])\n\n# Define the RNN layer\nrnn = SimpleRNN(64, dropout=0.2)(concatenated)\n\n# Define the output layer\noutput = Dense(1, activation='sigmoid')(rnn)\n\n# Define the model\nmodel = Model(inputs=[input_prompt, input_answer], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=30, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:57:51.058634Z","iopub.execute_input":"2023-02-22T11:57:51.059035Z","iopub.status.idle":"2023-02-22T11:58:31.823375Z","shell.execute_reply.started":"2023-02-22T11:57:51.059002Z","shell.execute_reply":"2023-02-22T11:58:31.822107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Input, concatenate\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Concatenate the embeddings\nconcatenated = concatenate([embedding_prompt, embedding_answer])\n\n# Define the hidden layers\ndense1 = Dense(32, activation='relu')(concatenated)\ndense2 = Dense(16, activation='relu')(dense1)\n\n# Define the output layer\noutput = Dense(1, activation='sigmoid')(dense2)\n\n# Define the model\nmodel = Model(inputs=[input_prompt, input_answer], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=30, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:58:31.824816Z","iopub.execute_input":"2023-02-22T11:58:31.825976Z","iopub.status.idle":"2023-02-22T11:58:43.107562Z","shell.execute_reply.started":"2023-02-22T11:58:31.825913Z","shell.execute_reply":"2023-02-22T11:58:43.106415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, concatenate, Input, Dropout\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Convolutional layers\nconv_prompt = Conv1D(32, 3, activation='relu')(embedding_prompt)\nconv_answer = Conv1D(32, 3, activation='relu')(embedding_answer)\n\n# Global max pooling layers\npooled_prompt = GlobalMaxPooling1D()(conv_prompt)\npooled_answer = GlobalMaxPooling1D()(conv_answer)\n\n# Concatenate the pooled layers\nconcatenated = concatenate([pooled_prompt, pooled_answer])\n\n# Dense layers with dropout\ndense = Dense(32, activation='relu')(concatenated)\ndropout = Dropout(0.5)(dense)\n\n# Define the output layer\noutput = Dense(1, activation='sigmoid')(dropout)\n\n# Define the model\nmodel = Model(inputs=[input_prompt, input_answer], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=30, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:58:43.109117Z","iopub.execute_input":"2023-02-22T11:58:43.109474Z","iopub.status.idle":"2023-02-22T11:58:59.078856Z","shell.execute_reply.started":"2023-02-22T11:58:43.109443Z","shell.execute_reply":"2023-02-22T11:58:59.077879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, concatenate, Input, Dropout\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Convolutional layers\nconv_prompt = Conv1D(32, 3, activation='relu')(embedding_prompt)\nconv_answer = Conv1D(32, 3, activation='relu')(embedding_answer)\n\n# Global max pooling layers\npooled_prompt = GlobalMaxPooling1D()(conv_prompt)\npooled_answer = GlobalMaxPooling1D()(conv_answer)\n\n# Concatenate the pooled layers\nconcatenated = concatenate([pooled_prompt, pooled_answer])\n\n# Dense layer with dropout regularization\ndense = Dense(32, activation='relu')(concatenated)\ndense = Dropout(0.2)(dense)\n\n# Define the output layer\noutput = Dense(1, activation='sigmoid')(dense)\n\n# Define the model\nmodel = Model(inputs=[input_prompt, input_answer], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=100, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T11:58:59.082097Z","iopub.execute_input":"2023-02-22T11:58:59.082614Z","iopub.status.idle":"2023-02-22T12:00:22.110674Z","shell.execute_reply.started":"2023-02-22T11:58:59.082577Z","shell.execute_reply":"2023-02-22T12:00:22.109475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, concatenate, Input\n\n# Define the input layers\ninput_prompt = Input(shape=(max_length,), name='input_prompt')\ninput_answer = Input(shape=(max_length,), name='input_answer')\n\n# Define the embedding layers\nembedding_prompt = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_prompt)\nembedding_answer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length)(input_answer)\n\n# Convolutional layers\nconv_prompt = Conv1D(32, 3, activation='relu')(embedding_prompt)\nconv_answer = Conv1D(32, 3, activation='relu')(embedding_answer)\n\nconv_prompt2 = Conv1D(64, 3, activation='relu')(embedding_prompt)\nconv_answer2 = Conv1D(64, 3, activation='relu')(embedding_answer)\npooled_prompt2 = GlobalMaxPooling1D()(conv_prompt2)\npooled_answer2 = GlobalMaxPooling1D()(conv_answer2)\nconcatenated2 = concatenate([pooled_prompt2, pooled_answer2])\ndense2 = Dense(64, activation='relu')(concatenated2)\noutput2 = Dense(1, activation='sigmoid')(dense2)\nmodel2 = Model(inputs=[input_prompt, input_answer], outputs=output2)\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel2.fit(x={'input_prompt': X_prompt, 'input_answer': X_answer}, y=train['AI'], batch_size=64, epochs=30, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:00:22.112417Z","iopub.execute_input":"2023-02-22T12:00:22.113010Z","iopub.status.idle":"2023-02-22T12:00:42.553538Z","shell.execute_reply.started":"2023-02-22T12:00:22.112966Z","shell.execute_reply":"2023-02-22T12:00:42.552578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"get the binary prediction from the best model then send it as a dataframe","metadata":{}},{"cell_type":"code","source":"# Tokenize the test data\nX_test_prompt = tokenizer.texts_to_sequences(test['prompt'])\nX_test_answer = tokenizer.texts_to_sequences(test['answer'])\n\n# Pad the sequences\nX_test_prompt = pad_sequences(X_test_prompt, maxlen=max_length, padding='post', truncating='post')\nX_test_answer = pad_sequences(X_test_answer, maxlen=max_length, padding='post', truncating='post')\n\n# Make predictions\ny_pred = model.predict(x={'input_prompt': X_test_prompt, 'input_answer': X_test_answer})\n\n# Convert predictions to binary (0 or 1)\ny_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:00:42.555939Z","iopub.execute_input":"2023-02-22T12:00:42.557224Z","iopub.status.idle":"2023-02-22T12:00:42.774879Z","shell.execute_reply.started":"2023-02-22T12:00:42.557175Z","shell.execute_reply":"2023-02-22T12:00:42.773899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_binary","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:00:42.776685Z","iopub.execute_input":"2023-02-22T12:00:42.777804Z","iopub.status.idle":"2023-02-22T12:00:42.790395Z","shell.execute_reply.started":"2023-02-22T12:00:42.777755Z","shell.execute_reply":"2023-02-22T12:00:42.789045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Category'] = y_pred_binary\ntest.drop(['prompt', 'answer'], axis=1, inplace=True)\ntest = test.rename(columns={'ID': 'Id'})","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:00:42.792203Z","iopub.execute_input":"2023-02-22T12:00:42.793262Z","iopub.status.idle":"2023-02-22T12:00:42.805760Z","shell.execute_reply.started":"2023-02-22T12:00:42.793214Z","shell.execute_reply":"2023-02-22T12:00:42.804288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:00:42.814414Z","iopub.execute_input":"2023-02-22T12:00:42.815267Z","iopub.status.idle":"2023-02-22T12:00:42.828099Z","shell.execute_reply.started":"2023-02-22T12:00:42.815225Z","shell.execute_reply":"2023-02-22T12:00:42.826929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.to_csv('/kaggle/working/out.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:06:44.196668Z","iopub.execute_input":"2023-02-22T12:06:44.197130Z","iopub.status.idle":"2023-02-22T12:06:44.204527Z","shell.execute_reply.started":"2023-02-22T12:06:44.197087Z","shell.execute_reply":"2023-02-22T12:06:44.203066Z"},"trusted":true},"execution_count":null,"outputs":[]}]}